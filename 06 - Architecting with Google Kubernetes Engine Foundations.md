## Introduction to Google Cloud



## Introduction to Containers and Kubernetes



## Kubernetes Architecture


Welcome and Getting Started Guide
Welcome to Architecting with Google Kubernetes Engine: Foundations! This course teaches you the basics for getting started with Kubernetes and Google Kubernetes Engine.

What is Kubernetes?

Kubernetes is an orchestration framework for software containers. Containers are a way to package and run code that's more efficient than virtual machines. Kubernetes provides the tools you need to run containerized applications in production and at scale.

What is Google Kubernetes Engine?

Google Kubernetes Engine (GKE) is a managed service for Kubernetes.  

What to expect during this training course?

During this training course, you will be presented with modules that will take you through skills you'll use as an architect or administrator of applications running in GKE. Each module contains video lectures, quizzes, and hands-on labs.

In order to get the best training experience, please ensure that you are prepared:

You should have a general familiarity with Google Cloud Platform; if you previously completed GCP Fundamentals: Core Infrastructure, you are well-positioned. You should also be comfortable working at the Linux command line and with a Linux text editor. You should be familiar with IT topics such as IP addresses, virtual machines, and web servers.

What's Next?

Watch the Course Introduction video lecture and then move onto the next lesson!


Hi, I'm Brian. Thanks for taking part in the architecting with Google Kubernetes Engine specialization. You're here because you're interested in Kubernetes, a software layer that sits between your applications and your hardware infrastructure. Because it abstracts away your underlying infrastructure, it's easier to consistently run and manage your applications. Google Kubernetes Engine brings you Kubernetes as a managed service on Google Cloud Platform. The specialization as a whole will teach you how to implement solutions using Google Kubernetes Engine, including, building, scheduling, load balancing, and monitoring workloads, as well as providing for discovery of services, managing role-based access control and security, and providing persistent storage to these applications. In this course, architecting with Google Kubernetes Engine foundation, each module aims to build on your ability to architect with GKE and includes hands-on labs for you to experience functionalities firsthand. In the first module, you'll be introduced to a range of Google Cloud platform services and features with a view to helping you choose the right GCP services to create your own Cloud solution. You'll learn about creating a container using Cloud Build and storing a container in Container Registry. You'll also compare and contrast the features of Kubernetes and Google Kubernetes Engine, also referred to as GKE. In addition to conceptualizing the Kubernetes architecture, you will deploy a Kubernetes cluster using GKE, deploy pods to a GKE cluster and view and managed Kubernetes objects.

Hello and welcome. I'm Philipp Maier, a course developer with Google Cloud Platform and this is a brief tutorial on using Qwiklabs in this course. I'm about to show you the interactive hands-on labs platform called Qwiklabs which is part of Google Cloud. Qwiklabs allows you to get practical hands-on experience with GCP and provisions you with Google account credentials so you can access that GCP console at no cost. The first step is to log into Coursera in an incognito window. Depending on your browser, it may also be called private browsing or InPrivate browsing. Logging into Coursera from a private window, ensures that you don't accidentally use your own Google account while accessing the Google Cloud Console. We don't want you to get any unexpected bills at the end of the month. Check out the links below this video for different browsers' support articles. Once logged into Coursera using any incognito window, return to your course and go to the lab activity page. So, if prompted, you want to accept the honor code and you might even have to enter your name. Then you want to click the open tool button to open the lab in a new tab. So, within the new tab, you can now click "start lab" and you want to wait until the "Lab Running" is displayed.
For each lab, you will have a timer in the top right with the remaining access time. Your lab will automatically end when the timer runs out. On the left, you have the connections detail. Click "Open Google Console" and then sign in with the username and password provided in the connections detail pane. So, I'm going to copy the username, paste that in here. I'm also going to take the password, paste it in here too. Now, Qwiklabs creates a new account for you each time you launch a lab. Therefore, you need to click through the initial account setup windows. So, essentially I need to accept this. I don't need to add any recovery phone numbers, so, I can just click DONE.
I'm going to agree to the terms and services, and I don't need any emails.
Now, I can verify that I'm using the Qwiklabs provided account and project within the GCP Console. So, up here, I see the project ID, and over here, I can see the username. These are the same ones that I was provided with in the connections detail pane. You can also see that the timer is still running. Now, some labs track your work within the Qwiklabs-provided GCP project. If this is enabled, you'll see a score in the top right corner of the Qwiklabs window as you can see right here. Your score increases as objectives are met and you can click on the score to view the individual steps to be scored. Now that I've completed the lab, I can see my score has been updated, and I'm ready to click End Lab. Once I click End Lab, the Qwiklabs-provided project and any resources within that project will be deleted.
I can close the Qwiklabs lab page and my grade will be updated with my lab score in Coursera. That's it for this tutorial. Remember to launch Coursera in an incognito window, and use the Qwiklabs-provided credentials to sign into the GCP Console. Good luck with the labs and enjoy the rest of this course.

Hi, I'm Evan. Welcome to introduction to Google Cloud platform. The first module of this course. I'm excited to share with you the stuff that you need to know, to use Google Kubernetes engine. And the other services that make up Google Cloud platform. In this first module, I'll share a simple explanation of what cloud computing is, so that we start off with the right framework of concepts. So here's a quick preview. Cloud computing entails resources being provided to you, as a service. GCP offers several services that let you run general purpose compute workloads on Google's hardware. And we're going to meet them. Google cloud platform as a global footprint. I'll explain how GCP's resources around the world, are organized into regions and zones. You can then use these resources. But of course it's your job to define the policies for what you want to use. GCP offers a hierarchical structure for organizing your use of cloud resources, and we'll meet it in this module. Finally, you'll meet the tools that let you connect to GCP, and allocate change in the listed resources.


Let's start by introducing the relationship between Cloud Computing and GCP. 
Cloud Computing has five fundamental attributes. 
First, computing resources are on-demand and self-service. Cloud computing customers using automated interface and get the processing power, storage, and network they need with no human intervention. 
Second, resources are accessible over a network from any location. Providers allocate resources to customers from a large pool, allowing them to benefit from economies of scale. 
Customers don't have to know or care about the exact physical location of these resources. 
Resources themselves are elastic. Customers who need more resources can get them rapidly, when they need less they can scale back. 
Finally, customers pay for only what they use or reserve as they go. If they stop using resources, they simply stop paying. That's how public Clouds, such as Google Cloud Platform work. 
GCP offers a variety of services and architects and developers like you, can choose among those services to build your solutions. Many of these Cloud services are very familiar, such as Virtual Machines, on-demand. Other services represent an entirely new paradigm. One of those services is Google Kubernetes Engine, and it's the focus of this course and the ones that follow. By the way, we informally call the service GKE and you can too. The first thing that many people ask of GCP is please run some my code in the Cloud. GCP provides a range of services for doing exactly that. Each aimed at satisfying a different set of user preferences. Here's a quick summary of those choices. 
In a later module, we'll compare each of them in greater detail. 
The service that may be most familiar to newcomers, is Compute Engine, which lets you run virtual machines on demand in the Cloud. It's Google Cloud's Infrastructure as a Service solution. It provides maximum flexibility for people who prefer to manage those server instances themselves. Now, GKE is different. It lets you run containerized applications on a Cloud environment that Google Cloud manages for you under your administrative control. 
What's a containerized application and what's Kubernetes? We'll learn a lot about each of those topics later on in this course. For now, you can think of containerization as a way to package code that's designed to be highly portable and to use resources very efficiently. You can think of Kubernetes is as a way to orchestrate code in those containers. 
App Engine is GCP's fully managed Platform as a Service Framework. That means it's a way to run code in the Cloud without having to worry about infrastructure. You can focus on just your code and let Google deal with all the provisioning and resource management. You can learn a lot more about App Engine in the specialization developing applications in Google Cloud Platform. 
Cloud Functions is a completely serverless execution environment or functions as a Service. It executes your code in response to events, whether those occur once a day or many times per second. Google scales resources as required, but you only pay for the service while your code runs. The specialization developing application of Google Cloud Platform, also discusses Cloud Functions. In this specialization, GKE is going to be our main focus, and GKE is built on top of Compute Engine, so we'll learn more about that service too along the way. 
Most applications need a database of some kind. If you've built a Cloud application, you can install and run your own database forward on a virtual machine inside of Compute Engine. You simply start up the virtual machine, install your database engine, set it up just like you would in a data center. Or you can run your own database server in Google Kubernetes too, however, with either approach, you have to manage and support the database yourself. 
Alternatively, you can use Google's fully managed database and storage services. What all of these have in common is that they reduce the work it takes to store all kinds of data. GCP offers relational and non-relational databases as well as worldwide Object Storage. You'll learn more about these later on in this specialization. GCP also offers fully managed and big data machine learning services. Just as with Storage and database services, you could build an implement these services yourself and some GCP customers do. But the fact that they're available as a service, means that you can get started faster and deal with a lot less routine work along the way.

In this video, you'll learn about the topology of GCP services and how they're distributed geographically in zones, regions, and at a global scale. You'll also learn how you can structure your resources you use hierarchically, using organizations, folders, and projects to manage your access control and your billing. Behind the services provided by Google Cloud Platform, lay a huge range of GCP resources, physical assets, such as physical servers and hard drives, and virtual resources such as virtual machines and containers. 
These resources are managed by Google within its global data centers. 
Google Cloud provides resources in multi-regions, regions, and zones. It divides the world up into three multi-regional areas. 
The Americas, Europe, and Asia Pacific. 
Next, the three multi-regional areas are divided into regions which are independent geographic areas on the same continent. 
Within a region, there's fast network connectivity, generally round-trip network latencies of under a millisecond. That's at the 95th percentile. 
Finally, regions are divided into zones, which are deployment areas for GCP resources within a focus geographic area. You can think of a zone as a data center within a region. Although strictly speaking, a zone isn't necessarily a single data center. 
We mentioned Compute Engine earlier. Compute Engine virtual machine instances reside within a specific zone. If that zone became unavailable so would your virtual machines and the workloads running in them, and GKE uses Compute Engine so your GKE workloads could be affected as well. Deploying applications across multiple zones enables fault tolerance and high availability. In this specialization, we'll learn how to do exactly that. 
Google's data centers around the world are interconnected by the Google network, which by some publicly available estimates, carries as much as 40 percent of the world's Internet traffic every day. This is the largest network of its kind on earth, and it continues to grow. It's designed to provide the highest possible throughput and the lowest possible latencies for applications including yours. 
The network interconnects with the public Internet and more than 90 Internet exchanges and more than 100 points of presence worldwide. When an Internet user sends traffic to a Google resource, Google responds to the user's request from an edge network location that will provide the lowest latency or delay. 
Google's edge casting network places the content close to users to minimize that latency. Your applications in GCP, including those running in GKE, can take advantage of this edge network too. When you take advantage of GCP services and resources, you get to specify those resources geographical locations. In many cases, you can also specify whether or not you're doing something at a zonal level or regional level or a multi-regional level. 
Zonal resources operate within a single zone, which means that if the zone becomes unavailable, the resources won't be available either. 
A simple example would be a Compute Engine virtual machine instance and its persistent disks. GKE has a component called the node and these are zonal too. Regional resources operate across multiple zones, but within one region. An application using these resources can be redundantly deployed to improve its availability. 
Later in this specialization, you'll learn how to use GKE so that its resources spread across different zones within a region.
Cloud Datastore is an example of another GCP service that can be deployed in a similar redundant way. Finally, global resources can be managed across multiple regions. These resources can further improve the availability of an application. 
Some example of such resources include HTTPS load balancers, and VPC or Virtual Private Cloud networks which GKE users benefit from too. The GCP resources you use, no matter where they reside, must belong to a project. What's a project? A project is the base level organizing entity for creating and using resources and services, and managing billing APIs, and permissions. 
Zones and regions physically organize a GCP resources you use and projects logically organize them. Projects can be easily created, managed, deleted, or even recovered from accidental deletions. 
Each project is identified by a unique project ID and project number. You can name your project and apply labels for filtering. These labels are changeable, but that project ID and project number remain fixed. 
Projects can belong to a folder, which is another grouping mechanism. You should use folders to reflect their hierarchy of your enterprise and apply policies at the right levels within your enterprise. Yes, I know what you're asking. You can nest folders inside of folders if you want. 
For example, you can have a folder for each department, and with each department's folder, you can have sub folders for each of the teams that make it up. Each team's projects belong to its folder. 
A single organization owns all the folders beneath it. 
An organization is the root node of a GCP resource hierarchy. Although you're not required to have an organization to use GCP, organizations are very very useful. Organizations let you set policies that apply throughout your entire enterprise. Also, having organization is required for you to use folders. 
If you're already a G Suite customer, you have an organization already and if not, you can get one for free through Google Cloud Identity, your organization and while the fixed organization ID and a changeable display name. The GCP resource hierarchy helps you manage resources across multiple departments and multiple teams within the organization. You can define a hierarchy that can create trust boundaries in resource isolation. 
For example, should members of your human resources team be able to delete running database servers and should your engineers be able to delete the database of employees salaries? Probably not in either case. 
Cloud Identity and Access Management also Cloud called IAM lets you fine tune access controls all the GCP resources you use. You define IAM policies that control user access to resources. You'll apply these policies at the level you choose, and those policies inherit downwards. 
For example, an IAM policy applied at the organizational level will be inherited by a folder, their project, and even the resources beneath it. Additional policies at lower levels of the hierarchy can grant additional permissions. Billing on the other hand, accumulates at the project level. 
Most GCP customers have a resource hierarchy that looks like their employee organization chart, while their project billing looks like their cost center structure. The resource hierarchy matters because of GCP's shared security model. When you build an application on your on-premises infrastructure, you're responsible for the entire stack security, for the physical security of the hardware, and the premises in which they're housed through the encryption of the data on disk, the integrity of your network all the way up to securing the content stored in those applications. 
But when you move an application to GCP, Google handles many of those lower levels of security, like the physical security of the hardware and its premises, the encryption of data on disk, and the integrity of the physical network. Because of its scale, Google can deliver a higher level of security at these layers than most customers could afford to on their own. The upper layers of the security stack, including the securing of the data, remain your responsibility.
Google does provide tools to help you implement the policies you can define at those layers. The resource hierarchy we just explored is one of those tools. Cloud IAM is another. You'll learn more about that security in depth later on in this specialization.

Billing, it's no fun. It's a fact of life. Let's learn more about it. Billing in GCP is set up at the GCP project level. When you define a GCP project, you'll link a billing account to it. This billing account is where you'll configure all your billing information, including your payment option. You can link your billing account to one or more projects. Projects that you don't link to any billing account could only use free GCP services. Your billing account can be charged automatically and invoiced every month, or at every threshold limit. You can separate project billings by setting up billing sub-accounts. Some GCP customers who resell GCP services use those sub-accounts for each of their own clients. And you're probably thinking how can I make sure I don't accidentally run up a big GCP bill? GCP provides three tools to help. Budgets and alerts, taking a look at the billing export, and also looking at your reports.
You can define budgets at the billing account level or even at the project level. To be notified when costs approach your budget limit, you can create an alert. For example, with a budget limit of $20,000 and an alert set at 90%, you'll receive a notification alert when your expenses reach that threshold or $18,000. You can also set up a webhook to be called in response to an alert. This webhook can control automation based on billing alerts. For example, you can trigger the script to shut down resources when a billing alert occurs. Or you can use this webhook to file a trouble ticket with your team.
The billing export allows you to store detailed billing information in places where it's easy to retrieve for external analysis, such as a BigQuery dataset or a Cloud Storage bucket.
And Reports is a visual tool in the console that allows you to monitor expenditure based on your projects and your services.
GCB also implements quotas which limit unforeseen extra billing charges. Quotas are designed to prevent the over consumption of resources because of an error or a malicious attack. Quotas apply at the level of the GCP project. Now, there are two types of quotas. Rate quotas and Allocation quotas. Rate quotas reset after a specific time. For example, by default, the GKE service implements a quota of 1,000 calls to its API for each GCP project every 100 seconds. After that 100 seconds, that limit is reset. Now, very important point. This doesn't limit the rate of calls to your applications running in GKE. But rather, calls to the administrative configuration of your GKE clusters themselves. It would be very unusual to make that many calls in such a short period of time. And that quota might help catch and stop that erroneous behavior. Allocation quotas govern the number of resources you can have in your projects. This doesn't reset at any interval. Instead, you need to free up those resources to stay within them. For example, by default, each GCP project has a quota allowing it no more than five VPC networks, or Virtual Private Clouds.
These quotas are not the same for all projects. Although projects start with the same quotas, you can change some of them by requesting an increase from Google Cloud Support. Some quotas may increase automatically based on your usage of a product. And you can use the GCP console to explicitly lower some of them for your own products. For example, if you wanted to put a more stringent cap on your consumption.
Finally, some quotas are fixed for all GCP customers. Regardless, in addition to the benefits to customers, GCP quotas also protect the community of GCP users by reducing the overall risk of unforeseen spikes in usage.

This next video looks at how you interact with GCP. You learned about the Google tools and interfaces that allow you to manage and configure your GCP resources. There are four ways to interact with GCP. 
The Google Cloud Platform Console, 
the Cloud shell and Cloud SDK, 
the Cloud Console mobile app, 
and REST-based APIs. 
Now we're not going to focus very much on APIs in this specialization. Developers use them to build applications that allocate and manage GCP resources. But our present focus is on letting Kubernetes manage those resources for us. 
The GCP console, so web based graphical user interface from where you manage your GCP resources. It allows you to execute common tasks using simple mouse clicks with no need to remember commands and avoiding typos. It also provides visibility into your GCP Project and its resources. Now you can sign into the GCP console from a web browser at console dot cloud dot Google dot com. All GCP services are accessible through the simple menu button in the top left hand corner. You can pin frequently used services to this menu. You'll learn how to use the GCP console during an upcoming lab. 
Alternatively, you can download and install the Google Cloud SDK onto a computer of your choice. The cloud SDK contains a set of command line tools for the Google Cloud platform, most notably for us. It contains the G Cloud and the kubeCTL commands which we will use a lot of in this course. 
It also contains the GSUtil and BQ utilities. You can run these tools interactively or in your automated scripts. The cloud SDK contains client libraries for various programming languages too. But what if it isn't convenient to install the cloud SDK on the machine that you're working with? 
Cloud shell provides command line access to your cloud resources directly from within your web browser. Using cloud shell you can manage your projects and resources easily without having to install the cloud SDK or other tools locally. 
The cloud SDK, G Cloud and kubeCTL Command line tools and other utilities are always available, up to date and fully authenticated. So how does cloud shell do that? It's built using a compute engine virtual machine instance that you're not build for, each GCP user has one. 
Your cloud shell virtual machine is ephemeral, which means it will be stopped whenever you stop using interactively. It'll be restarted for you every time you re enter cloud shell. So you wouldn't want to run a production Web Server in your cloud shell for example. You'll also get 5 gigabytes of persistent disk storage that is reattached for you every time a new cloud shell session is started. 
It also provides a Web Preview functionality and built in authorization process for access to gcp console projects and resources. Including your GKE resources. The cloud shell code editor is a tool for editing files inside of your cloud shell environment in real time with in your web browser. You can also use text editors from within the cloud Shell command prompt. This tool is extremely convenient and working with code first applications or container based workloads. 
Because you can easily edit files without the need to download and upload changes. But is the easy way always the best way? Of course not, later on, this specialization will talk about the best management practices for these code files. Here's a screenshot of the GCP console's GKE area, showing you its web based interface for administering your GKE resources. The bottom third of the screenshot is your cloud shell and operation. 
Where you can launch commands to administer those resources as well. Some of those commands are from the cloud, Google cloud SDK and others will be specific to your workload. Later in this course we'll learn about the kubeCTL Command and you can see it being launched from cloud shell here. 
Finally, there's the Cloud Console mobile app available for iOS and Android. It offers many capabilities like managing virtual machines and viewing their logs. Getting up to date billing information for your projects, getting billing alerts for projects that are going over budget. And setting up customizable graphs, showing key metrics such as CPU usage, network usage, requests per second in any server errors. We'll not use it in course, but it's a resource that you might find convenient. Of course it has no additional change.

Now, we'll start off in the GCP console. Let's go to the home screen and select our project name. We'll use this later.
Now, let's go to the storage browser and create a new bucket. We're going to name this bucket after our GCP project. We'll accept all the defaults.
Now, let's create a Virtual Machine Instance. We'll click the Create Instance button,
and we're going to change the name of the VM to first-vm.
Let's run it in the us-central1-c zone.
Let's compare some of the prices of machine types. The micro type is very inexpensive. We'll take almost all the other defaults except, we're going to configure the firewall to allow HTTP traffic through.
Now we click Create. It takes a few minutes to create the virtual machine.
Once the virtual machine has been created, let's perform a different task. We're going to make a service account.
We click create service account and give the account name.
When we click Create, we get the opportunity to define a role for the account. We'll give this one the editor role.
A key allows actors to authenticate as the service account.
It's downloaded to our local computer. We'll upload it later in the lab.
Now that the account has been created, it shows up in the list.
Next, let's explore Cloud Shell.
Cloud Shell gives us an environment where we can use the G-Cloud command and other cloud commands from the G-Cloud SDK. We're already authenticated.
Our first task is to define some environment variables for convenience. It's not required, but it helps us avoid error.
We're defining my bucket name one to be the same as our GCS bucket that we created earlier which is named after our project. We'll create another name for a bucket by using that name with a dash two appended to it.
We'll echo the contents of that environment variable to make sure it works. I'm going to full-screen the Cloud Shell window and upload the JSON credentials file. It's underneath the three-dot menu.
Let's check to make sure it worked.
There it is, the file's been successfully uploaded. We'll use it later in the lab. Let's use the gsutil command to make a second cloud storage bucket. Gsutil mb does the job. It's created. When we go back to the storage browser, we will see both the new bucket and the old one in the list. The older one first and then the new one. Let's return to Cloud Shell and make a second virtual machine instance, this time using the command line. Again, to save ourselves the risk of error, we're going to use environment variables. We'll define an environment variable to contain our chosen region. Now let's search for the zones in that region.
Looks like the us-central1 region has four zones. With this VM, we're going to choose the a zone. We'll use the G-Cloud command to define our preferred zone, and we'll put our preferred name for our virtual machine into another environment variable for tidiness. Here's the G-Cloud command to create a virtual machine from the command line. As always, it takes a moment to complete.
Now that it's done, we can see it in the output of the G-Cloud compute instances list command as well as in the GCP Console.
In the Compute Engine VM instances list, we see first VM and second VM. There's also VM here that is an artifact of the Quick Labs environment, that you won't see in your own projects.
Let's see how to create a service account from the command line. Let's go to the GCP console now, and see our newly created service account in the list.
Under IAM & admin, you go to service accounts and scroll down.
There it is. Now we're going to use the command line to give the second service account the project viewer role.
Done. We can go into the GCP Console and confirm the effective or action. You click on "IAM" and scroll down in the list of members to find the newly created service account. Its last in alphabetical order. We click the pencil icon and that reveals that it has the viewer role.
Now let's work with Cloud Shell. This command copies a picture of a cat out of the publicly available Cloud storage bucket into our Cloud Shell folder.
Now we're going to copy that file into our first storage bucket.
Now we're going to copy it from one storage bucket to another. All of these were done using the gsutil cp command. Now each of our two bucket has a copy of this picture of a cat. Using the gsutil acl command, we're going to configure the first bucket so that it is private to only its owner.
We can see the effect of this change by changing who we are authenticated as. The gcloud config list command shows us that presently the account we're authenticated as is gcpstaging57838_studentqwiklabs.net. Let's change that. Now we're going to assume the role of a service account. The first one we created. Notice that we use the credentials file that we previously uploaded. The gcloud config list command confirms the change is effect. Notice the account now is shown as our service account. Now when we try to copy the cat picture into the first storage bucket, we get an access denied exception because it is private to the owner and we are in the service account now instead of the owner.
We can copy the image into the second bucket because we didn't change its permissions. Let's change who we're authenticated as back to our GCP account.
We will use the gcloud config command just like before.
Now let's repeat our attempt to copy the cat picture into the bucket. It succeeds because we own the bucket. Now let's configure that bucket so that its contents are public.
Now when we look at the bucket using the storage browser of the GCP console, we'll notice something new.
We'll drill into that bucket and there we will see that there's a public link for the cat picture. We'll open in new tab and we see a picture of a cat. We will use this in a web page. Remember, we configured a virtual machine earlier and opened its firewall port for HTTP. We'll create the web page using the Cloud Shell code editor which you can launch by clicking the pencil icon. Let's explore the Cloud Shell editor first.
First, let's experiment with what happens when we change the contents of our Cloud Shell home directory. We're going to clone a Git repository. Notice that the change is immediately reflected in the file display. When I create a directory from the command line, that change is reflected immediately too. Let's open up one of these directories and open a file. Here's the file, cleanup.sh. I'm going to add a new line at the very end of this file.
The change is immediately visible from the command line. I'm going to cap the file and there's the line I added. Now let's go make that HTML page. I'm going to return to the home directory and my HL prompt and I'm going to go to the File menu of the editor and create a file called index.html.
I'm going to paste in some web page text and I'm going to replace the URL with the actual URL of the copy of the cat picture that I put into my public Cloud storage bucket.
Now I'm going to copy that file out of Cloud Shell into my virtual machine. We need to install web server software into our virtual machine. So we're going to SSH to it using the link that's provided for us in the VM instances display.
We'll use the standard Debian Linux commands to install the Nginx web server.
Now that that's done let's return to Cloud Shell and move our HTML file in using SCP.
I know why that didn't work. I put the file into the wrong directory. I'm going to use Cloud Shell's editor to drag it into the right place, into my home directory. Now the command will work.
The file is copied to my virtual machine. To return to my SSH window on the VM and move that HTML file from the home directory to Nginx's document root. Now I can go to the VM Instances screen and click the link and I get the web page that I created.

That concludes the introduction to Google Cloud Platform module. Let me remind you of what you've learned. Cloud computing is a way to organize your use of IT, in which a provider gives you on demand access over the network to resources from a pool that they maintain. You pay for what you use or reserve. The provider maintains the infrastructure for you, and you can turn it off when you're done. Google cloud platform offers lots of different kinds of cloud computing resources. Including four that run your code for you on Google hardware. In this course. We're focusing on Kubernetes engine. All the resources offered by GCP are organized into regions and zones. You can use resources in several zones in a region to increase your application's resiliency. GCP is a shared security model. You're responsible for defining security policies for your GCP resources and the cloud resource management. Hierarchy helps you do that in a manageable way. Two important tools that you can use to manage your use of GCP resources are the GCP Console and cloud shell. We'll be using both of those throughout the rest of this specialization

Welcome to the introduction to containers and kubernetes module. In this module, you'll learn what containers are, what their benefits are for application deployment, how containers are configured in built, what functions container management solutions like kubernetes provide, and what the advantage of Google kubernetes engine are, compared to building your own container management infrastructure. In this module, you'll learn how to create a container using Cloud Build and store the container and get in a container registry, and then compare and contrast kubernetes using Google kubernetes engine features.

Let's start by introducing containers. In this video, you'll learn about the key features of containers and the advantages of using containers for application deployment compared to alternatives such as deploying apps directly to virtual machines. 
You'll learn about Google's Cloud Build, and then you can see how you can use it to build and manage your application images. It's now not very long ago, the default way to deploy an application was on its own physical computer. 
To set one up, you'd find some physical space, power, cooling, network connectivity for it, and then install an operating system, any software dependencies, and then finally the application itself. 
If you need more processing power, redundancy, security, or scalability, what'd you do? Well you'd have to simply add more computers. It was very common for each computer to have a single-purpose. 
For example, a database, web server, or content delivery. This practice as you might imagine, wasted resources and it took a lot of time to deploy and maintain and scale. It also wasn't very portable at all. Applications were built for a specific operating system and sometimes even for specific hardware as well. In comes the dawn of virtualization. 
Virtualization helped by making it possible to run multiple virtual servers and operating systems on the same physical computer. A hypervisor is the software layer that breaks the dependencies of an operating system with its underlying hardware, and allow several virtual machines to share that same hardware. KVM is one well-known hypervisor. Today you can use virtualization to deploy new servers fairly quickly. Now adopting virtualization means that it takes us less time to deploy new solutions, we waste less of the resources on those physical computers that we're using, and we get some improved portability because virtual machines can be imaged and then moved around. 
However, the application, all of its dependencies and operating system are still bundled together and it's not very easy to move from a VM from one hypervisor product to another. 
Every time you start up a VM, it's operating system still takes time to boot up. 
Running multiple applications within a single VM also creates another tricky problem, applications that share dependencies are not isolated from each other, the resource requirements from one application, can starve out other applications of the resources that they need. Also, a dependency upgrade for one application might cause another to simply stop working. You can try to solve this problem with rigorous software engineering policies. 
For example, you could lock down the dependencies that no application is allowed to make changes, but this leads to new problems because dependencies do need to be upgraded occasionally. You can add integration tests to ensure that applications work. 
Integration tests are great, but dependency problems can cause new failure modes that are harder to troubleshoot, and it really slows down development if you have to rely on integration tests to simply just perform basic integrity checks of your application environment. 
Now, the VM-centric way to solve this problem is to run a dedicated virtual machine for each application. Each application maintains its own dependencies, and the kernel is isolated. So one application won't affect the performance of another. One you can get as you can see here, is two complete copies of the kernel that are running. But here too we can run into issues as you're probably thinking. 
Scale this approach to hundreds of thousands of applications, and you can quickly see the limitation. Just imagine trying to do a simple kernel update. So for large systems, dedicated VMs are redundant and wasteful. 
VMs are also relatively slow to start up because the entire operating system has to boot. A more efficient way to resolve the dependency problem is to implement abstraction at the level of the application and its dependencies. You don't have to virtualize the entire machine or even the entire operating system, but just the user space. 
Again, the user space is all the code that resides above the kernel, and includes the applications and their dependencies. This is what it means to create containers. 
Containers are isolated user spaces per running application code. Containers are lightweight because they don't carry a full operating system, they can be scheduled or packed tightly onto the underlying system, which is very efficient. They can be created and shut down very quickly because you're just starting and stopping the processes that make up the application and not booting up an entire VM and initializing an operating system for each application. 
Developers appreciate this level of abstraction because they don't want to worry about the rest of the system. 
Containerization is the next step in the evolution of managing code. You now understand containers as delivery vehicles for application code, they're lightweight, stand-alone, resource efficient, portable execution packages. You develop application code in the usual way, on desktops, laptops, and servers. The container allows you to execute your final code on VMs without worrying about software dependencies like application run times, system tools, system libraries, and other settings. You package your code with all the dependencies it needs, and the engine that executes your container, is responsible for making them available at runtime. 
Containers appeal to developers because they're an application-centric way to deliver high performance and scalable applications. 
Containers also allow developers to safely make assumptions about the underlying hardware and software. With a Linux kernel underneath, you no longer have code that works in your laptop but doesn't work in production, the container's the same and runs the same anywhere. You make incremental changes to a container based on a production image, you can deploy it very quickly with a single file copy, this speeds up your development process. 
Finally, containers make it easier to build applications that use the microservices design pattern. That is, loosely coupled, fine-grained components. This modular design pattern allows the operating system to scale and also upgrade components of an application without affecting the application as a whole.

An application and its dependencies are called an image. A container is simply a running instance of an image. By building software into Container images, developers can easily package and ship an application without worrying about the system it will be running on. 
You need software to build Container images and to run them. Docker is one tool that does both. Docker is an open source technology that allows you to create and run applications in containers but it doesn't offer a way to orchestrate those applications at scale like Kubernetes does. In this course, we'll use Google's Cloud build to create Docker formatted Container images. Containers are not an intrinsic primitive feature of Linux. Instead their power to isolate workloads is derived from the composition of several technologies. 
One foundation is the Linux process. Each Linux process has its own virtual memory address phase, separate from all others. Linux processes are rapidly created and destroyed. 
Containers use Linux namespaces to control what an application can see, process ID numbers, directory trees, IP addresses, and more. By the way, Linux namespaces are not the same thing as Kubernetes Namespaces, which you'll learn more about later on in this course. 
Containers use Linux cgroups to control what an application can use, its maximum consumption of CPU time, memory, IO bandwidth, other resources. 
Finally, Containers use Union File Systems to efficiently encapsulate applications and their dependencies into a set of clean minimal layers. Now let's see how that works. 
A container image is structured in layers. The tool you use to build the image reads instructions from a file called, ''The Container manifest.'' 
In the case of a Docker formatted Container Image, that's called a Dockerfile. 
Each instruction in the Docker file specifies a layer inside the container image. Each layer is, ''Read only.'' When a Container runs from this image, it will also have a writable ephemeral top-most layer. Let's take a look at a simple Docker file. This Dockerfile will contain four commands, each of which creates a layer. At the end of this discussion, I'll explain why this Docker file is a little oversimplified for modern use. 
The From statement starts out by creating a base layer pulled from a public repository. This one happens to be the Ubuntu Linux runtime environment of a specific version. 
The Copy command adds a new layer containing some files copied in from your build tools current directory. 
The Run command builds your application using the make command and puts the result of the build into a third layer. 
Finally, the last layer specifies what command to run within the container when it's launched. 
Each layer is only a set of differences from the layer before it. When you write a Docker file, you should organize the layers least likely to change through to the layers that are most likely to change. By the way, I promised that I'd explain how the Docker file example you saw here is oversimplified. These days, the best practice is not to build your application in the very same container that you ship and run. After all, your build tools are at best just cluttered and deployed Container and at worst, are an additional attack service. 
Today, Application Packaging relies on a multi-stage build process in which one Container builds the final executable image. A separate container receives only what's needed to actually run the application. Fortunately for us, the tools that we use support this practice. 
When you launch a new container from an image, the Container Runtime adds a new writable layer on the top of the underlying layers. This layer is often called the Container layer. All changes made to the running container, such as writing new files, modifying existing files, and deleting files are written to this thin writable Container layer in their ephemeral, when the Containers deleted the contents of this writeable layer are lost forever. The underlying Container Image itself remains unchanged. 
This fact about Containers has an implication for your application design. Whenever you want to store data permanently, you must do so somewhere other than a running container image. You'll learn that in the several choices that you can choose from in this specialization. 
Because each Container has its own writable Container layer and all changes are stored in this layer. Multiple Containers can share access to the same underlying image and yet have their own data state. The diagram here shows multiple containers sharing the same Ubuntu 15.04 image. 
Because each layer is only a set of differences from the layer before it, you get smaller images. For example, your base application image, maybe 200 megabytes, but the difference, the next point release might only be 200 kilobytes. 
When you build a container, instead of copying the whole image, it creates a layer with just the differences. When you run a container, that Container Runtime pulls down the layers it needs. 
When you update, you only need to copy the difference, this is much faster than running a new virtual machine. It's very common to use publicly available open source Container images as a base for your own images or for unmodified use. For example, you've already seen the Ubuntu Container Image, which provides an Ubuntu Linux environment inside of a container. 
Alpine is a popular Linux environment in a container, noted for being very, very small. The NginX web server is frequently used in its Container packaging. Google maintains a Container Registry, gcr.Io. This Registry contains many public, open source images and Google Cloud customers also use it to store their own private images in a way that integrates well with Cloud IAM. 
Google Container Registry is integrated with Cloud IAM. So for example, you can use it to store your images that aren't public. Instead, they're private to your project. 
You can also find Container images and other public repositories, Docker Hub Registry, GitLab and others. The open source Docker command is a popular way to build your own Container images. It's widely known and widely available. 
One downside, whoever a building Containers with a Docker command is that you must trust the computer that you do your builds on. 
Google provides a managed service for building Containers that's also integrated with Cloud IAM. This service's called, ''Cloud Build,'' and we'll use it in this course. Cloud build can retrieve the source code for your builds from a variety of different storage locations. Cloud Source Repositories, Cloud Storage, which is GCP is Object Storage service or git compatible repositories like GitHub and Bitbucket to generate a buildup Cloud Build, you define as series of steps. For example, you can configure build steps to fetch dependencies, compile source code, run integration tests, or use tools such as Docker, Gradle, and Maven. Each build step and Cloud build runs in a Docker container. Then Cloud build can deliver your newly built images to various execution environments, not only GKE, but also App Engine and Cloud Functions.

In this Lab, you'll build a Docker container image from provided source code and a Docker file using Cloud Build. You'll then upload the container to contain a registry. The tasks that you'll perform include: using Cloud build to build and push containers and then using container Registry to store and deploy containers. Remember that you have multiple tries to complete the Lab, so don't worry if your lab timer runs out. You can always start again. Now, try out the Lab and come back and watch the solution walkthrough video where we'll highlight each of the critical points you just practiced.

Let's start by making sure that the needed APIs are enabled.
In the GCP console, we go to APIs and Services.
The API usage graphs are empty because this is a new project. We click enable APIs and Services and type in the name of the GPI we needed. First, we need to check Cloud Build.
It says API enabled, that's great. Now, let's go back and try the other API we need for this activity, the container-registry API.
It's also enabled. We would have otherwise been shown a button to enable the API.
Now let's launch Cloud Shell.
We'll use the simple nano text editor to create a short shell script, this will be the useful payload of the container we built.
This shell script simply prints a message whenever it is run.
We press Cmd+X to exit and save.
Now we create a simple Dockerfile.
The base layer of our container will be Alpine Linux. We'll place our shell script into the root directory of the container, and it will arrange for it to be run when the container is launched. We press Cmd+X to exit.
We'll set the permissions of the container so that it is executable.
Now we'll call cloud build to build the container image.
The build succeeded, so let's go look at it in Google Container Registry.
Here's our container, it's name is what we called it on the gcloud build command-line.
Now we're working with two variations on this pattern. Their sources are stored in a Git repository to save your typing, we claim the menus and get claim.
Here's a YAML file that describes our build. We're going to create an image with the same name as before. Notice that the Dockerfile and the shell script are here just like before. We submit the YAML file to cloud build
The build succeeded.
Now let's go see how this is reflected in Google Container Registry. We refresh the display.
Still an image by the name quickstart-image, but when we click on it we see that there's an older version and a newer version.
Cloud Build>History records the process of building this container images.
A powerful capability is cloud build is it's ability to include large cloud build
Let's take a look at a different YAML file.
This YAML file runs the just built container image.
You could use this capability to run a test suit that's embedded in your container image. In this example, we have simulated test with that always fails. We launch the build once again.
The test suite fails, so the build fails. The status is reported to the calling shell.

Now, let's introduce a popular container management and orchestration solution called Kubernetes. Let's say your organization is really embraced the idea of containers. Because containers are so lean, your coworkers are creating them in numbers far exceeding the counts of virtual machines you used to have and the applications running in them need to communicate over the network. But you don't have a network fabric that lets containers find each other. You need help. 
How can you manage your container infrastructure better? Kubernetes is an open source platform that helps you orchestrate and manage your container infrastructure On-premises or in the Cloud. So what is Kubernetes? It's a container centric management environment. Google originated it and then donated it to the open source community. Now, it's a project of the Vendor Neutral Cloud Native Computing Foundation. 
It automates the deployment scaling, load balancing, logging, monitoring, and other management features of containerized applications. These are the features that are characteristic of a typical platform as service solutions. Kubernetes also facilitates the features of an infrastructure as a service, such as allowing a wide range of user preferences and configuration flexibility. Kubernetes supports declarative configurations. 
When you administer your infrastructure declaratively, you describe the desired state you want to achieve instead of issuing a series of commands to achieve that desired state. Kubernetes job is to make the deployed system conform to your desired state and then keep it there in spite of failures. Declarative configuration saves you work. Because the system is desired state is always documented, it also reduces the risk of error. 
Kubernetes also allows imperative configuration in which you issue commands to change the system state. But administering Kubernetes as scale imperatively, will be a big missed opportunity. One of the primary strengths of Kubernetes is its ability to automatically keep a system in a state that you declare. Experienced Kubernetes administrators use imperative configuration only for quick temporary fixes and as a tool in building a declarative configuration. Now that you know what Kubernetes is, let's talk about some of its features. 
Kubernetes supports different workload types. It supports stateless applications such as an Nginx or Apache web server, and stateful applications where user in session data can be stored persistently. It also supports batched jobs and demon tasks. Kubernetes can automatically scale in and out containerized applications based on resource utilization. You can specify resource requests levels and resource limits for your workloads and Kubernetes will obey them. These resource controls like Kubernetes, improve overall workload performance within the cluster. 
Developers extend Kubernetes through a rich ecosystem of plugins and add-ons. For example, there's a lot of creativity going on currently with Kubernetes custom resource definitions which bring the Kubernetes declarative Management Model to amazing variety of other things that need to be managed. The primary focus of this specialization though, is architecting with Kubernetes because it's provided as a service by Google Cloud. So extending Kubernetes is not within our scope. Because it's open source, Kubernetes also supports workload portability across On-premises or multiple Cloud service providers such as GCP and others. This allows Kubernetes to be deployed anywhere. You can move Kubernetes workloads freely without a vendor login.

Google cloud's managed service offering for Kubernetes is called Google, Kubernetes Engine or GKE. 
So why do people choose it? What if you begun using Kubernetes in your environment, but the infrastructure has become too much of a burden for you to maintain? Is there anything within Google cloud platform that can help you? Absolutely, totally, that's going to be Google Kubernetes Engine or GKE. GKE, let's talk about what it can do. 
It will help you deploy, manage and scale Kubernetes environments for your containerized applications on GCP. More specifically, GKE is a component of the GCP compute offerings. 
It makes it easy to bring your Kubernetes workloads into the cloud. GKE is fully managed, which means that you don't have to provision the underlying resources. GKE uses a container-optimized operating system. These operating systems are maintained by Google. And they're optimized to scale quickly and with a minimal resource footprint. 
The container-optimized OS, it will be discussed later on in this course. When you use GKE, you start by directing the service to instantiate a Kubernetes system for you. This system is called a cluster. GKE's auto upgrade feature can be enabled to ensure that your clusters are automatically upgraded with the latest and greatest version of Kubernetes. The virtual machines that host your containers inside of a GKE cluster are called nodes. 
If you enable GKE's auto repair feature, the service will automatically repair unhealthy nodes for you. It will make periodic health checks on each node in the cluster. 
If a node is determined to be unhealthy and requires repair, GKE would drain the node. In other words, it will cause it's workloads to gracefully exit and then recreate that node. 
Just as Kubernetes support scaling workloads, GKE support scaling the cluster itself. GKE seamlessly integrates with Google Cloud build and container registry. This allows you to automate deployment using private container images that you've securely stored in container registry. 
GKE also integrates with Google's identity and access management, which allows you to control access through the use of accounts and role permissions. 
Stackdriver is Google Cloud system for monitoring and management for services, containers, applications, and infrastructure. GKE integrates with Stackdriver monitoring to help you understand your applications performance. 
GKE is integrated with Google virtual private clouds or VPCs, it makes use of GCP's networking features. And finally, the GCP console provides insights into GKE clusters and the resources, and it allows you to view, inspect and delete resources in those clusters. You might be aware that open source Kubernetes contains a dashboard, but it takes a lot of work to set it up securely. But the GCP console is a dashboard for GKE clusters and workloads that you don't have to manage. And it's more powerful than the Kubernetes dashboard.

In this last lesson, you learn more about the computing options available. 
In a previous module, I briefly introduced your choices for running compute workloads in GCP. Now that we know more about how containers work, we can compare these choices in more detail. 
The services are 
  - Compute Engine, 
  - GKE, 
  - App Engine, 
  - Cloud Run, 
  - Cloud Functions.
At the end of this lesson, you'll understand why people choose each. Compute Engine offers virtual machines that run on GCP. You can select predefined VM configurations. 
At the time this course was developed, these virtual machines could be as large as 160 V CPUs with more than 3 terabytes of memory. You could also create customized configurations to precisely match your performance and cost requirements. Virtual machines need block storage. 
Compute Engine offers you two main choices, persistent disks and local SSDs. Persistent disks offer network stores that can scale up to 64 terabytes and you can easily take snapshots of these disks for backup and mobility. You could also choose local SSDs which enable very high input/output operations per second. 
You can place your Compute Engine workloads behind global load balancers that support autoscaling. 
Compute Engine offers a feature called managed instance groups. With these you can define resources that are automatically deployed to meet demand. GCP enables fine grained control of costs of Compute Engine resources by providing per second billing. This granularity helps reduce your costs when deploying compute resources for short periods of time, such as batch processing jobs. Compute Engine offers preemptible virtual machines which provide significantly cheaper pricing for your workloads that can be interrupted safely. 
So why do people choose Compute Engine? With Compute Engine you have complete control over your infrastructure. You can customize operating systems and even run applications that rely on a mix of operating systems. You can easily lift and shift your on-premises workloads into GCP without rewriting your applications or making any changes. 
Compute Engine is the best option when other computing options don't support your applications or requirements.
App Engine has a completely different orientation from Compute Engine. App Engine is a fully managed application platform. Using App Engine means zero server management and zero configuration deployments. 
So if you're a developer, you can focus on building applications and not really worrying about the deployment part. So you can simply use your code and App Engine will deploy that required infrastructure for you. 
App Engine supports popular languages like Java and Node.js, Python, PHP, C#, .NET, Ruby, and Go. You could also run container workloads. Stackdriver monitoring, logging, and diagnostics, such as debugging and error reporting are also tightly integrated with App Engine. You can use Stackdriver's real time debugging features to analyze and debug your source code. Stackdriver integrates with tools such as Cloud SDK, cloud source repositories, IntelliJ, Visual Studio, and PowerShell. App Engine also supports version control and traffic splitting.
App Engine is a good choice if you simply want to focus on writing code, and you don't want to worry about building the highly reliable and scalable infrastructure that'll run on. 
You can just focus on building applications instead of deploying and managing the environment. Use cases for App Engine include websites, mobile apps, gaming backends, and as a way to present a RESTful API to the Internet. What's a RESTful API? In short, it's an application program interface that resembles the way a web browser interacts with the web server. RESTful APIs are easy for developers to work with and extend. 
And App Engine makes them easy to operate. 
Finally, the main topic of this course, Google Kubernetes Engine. We learned that Kubernetes is an orchestration system for applications in containers. It automates deployment, scaling, load balancing, logging, and monitoring, and other management features. Google Kubernetes Engine extends Kubernetes management on GCP by adding features and integrating with other GCP services automatically. 
GKE supports cluster scaling, persistent disks, automated updates to the latest version of Kubernetes, and auto repair for unhealthy nodes. It has built-in integration with cloud build, container registry, Stackdriver monitoring, and Stackdriver logging. Existing workloads running within on-premise clusters can easily be moved on to GCP. There's no vendor login. Overall, GKE is very well suited for containerized applications. 
Cloud-native distributed systems and hybrid applications. Kubernetes and GKE are discussed in depth throughout this course.
Web requests, or cloud Pub/Sub events. 
Cloud Run is serverless, it distracts way all the infrastructure management so you can focus on developing applications. It's built on Knative, an open source Kubernetes based platform. It builds, deploys, and manages modern stateless workloads. 
Cloud Run gives you the choice of running your containers easier with fully managed or in your own GKE cluster. Cloud Run enables you to run request or event driven stateless workloads without having to worry bout servers. It abstracts away all the infrastructure management such as provisioning, configuring, managing those servers so you can focus on just writing code. It automatically scales up and down from zero depending upon traffic almost instantaneously, so you never have to worry about scale configuration. 
Cloud Run charges you for only the resources that you use calculated down to the nearest 100 milliseconds. So you don't have to pay for those over provisioned resources. 
With Cloud Run you can choose to deploy your stateless containers with a consistent developer experience to a fully managed environment or to your own GKE cluster. This common experiences enabled by Knative an open API and runtime environment built on top of Kubernetes. And it gives you the freedom to move your workloads across different environments and platforms, either fully managed on GCP, on GKE or anywhere a Knative runs. 
Cloud Run enables you to deploy stateless containers that listen for requests or events delivered via HTTP requests. With Cloud Run, you can build your applications in any language using whatever frameworks and tools you wish and deploy them in seconds without having to manage and maintain that server infrastructure.
Cloud Functions is an event-driven, serverless compute service for simple single purpose functions that are attached to events. In Cloud Functions, you simply upload your code written in JavaScript or Python, or Go and then GCP will automatically deploy the appropriate computing capacity to run that code. 
These servers are automatically scaled and are deployed from highly available and a fault-tolerant design. You're only charged for the time that your code runs. For each function, invocation memory and CPU use is measured in the 100 millisecond increments, rounded up to the nearest increment. 
Cloud Functions also provides a perpetual free tier. So many cloud function use cases could be free of charge. With Cloud Functions, your code is triggered within a few milliseconds based on events. For example, a file is uploaded to Google cloud storage or a message is received from Cloud Pub/Sub. 
Cloud Functions can also be triggered based on HTTP endpoints that you define, and events in the fire based mobile application back end. What are some of the use cases for Cloud Functions? They're generally used as part of a microservices application architecture. You can also build symbols, serverless, mobile IoT backends, or integrate with third party services and APIs. Files uploaded into your GCS bucket can be processed in real time. Similarly, the data can be extracted, transformed and loaded for querying in analysis. 
GCP customers often use Cloud Functions as part of intelligent applications, such as virtual assistance, video or image analysis, and sentiment analysis. So which compute service should you adopt? A lot depends on where you're coming from. If you're running applications on physical server hardware, it will be the path of least resistance to move into compute engine. What if you're running applications in long-lived virtual machines in which each VM is managed and maintained? In this case, you'll also find moving to compute engine is the quickest GCP services for getting your applications to the cloud. 
What do you want to to think about operations at all? 
Well, App Engine and Cloud Functions are good choices. You can learn more about the differences between App Engine and Cloud Functions in the courses under a developing applications with Google Cloud platform. I hope that this course so far is help you understand why software containers are so beneficial. 
Containerization is the most efficient, importable way to package you an application. The popularity of containerization is growing very fast. In fact, both Compute Engine and App Engine can launch containers for you. 
Compute Engine will accept the container image from you and launch a virtual machine instance that contains it. You can use Compute Engine technologies to scale and manage the resulting VM. 
And App Engine flexible environment will accept the container image from you and then run it with the same No-ops environment that App Engine delivers for code. 
But what if you want more control over your containerized workloads than what App Engine offers? And denser packing than what Compute Engine offers? That increasingly popular use case is what GKE is designed to address. 
The Kubernetes paradigm of container orchestration is incredibly powerful, and its vendor neutral, and a abroad and vibrant community is developed all around it. Using Kubernetes as a managed service from GCP saves you work and let's you benefit from all the other GCP resources too. 
You can also choose Cloud Run to run stateless containers on a managed compute platform. And of course, if you're already running Kubernetes in your on-premises data centers, moving your GKE is a great choice. Because you'll be able to bring along both your workloads and your management approach.

That concludes introduction to containers and Kubernetes. In this module, you learned how to create a container using cloud build, which will build and manage your application images. In your labs, you create a Docker formatted container images and so you could privately store them in container registry. You also learn how to securely manage your builds with the native integration with cloud IM. You review the four compute solutions offered by Google Cloud platform starting with compute engine for those lift and shift workloads, app engine, GKE, and cloud functions. Then you weighed the benefits of each and looked at some common use cases. Lastly, you learn that GKE supports automatic Kubernetes version updates, automatic repair for unhealthy nodes, and even scaling of the cluster itself for you. All of this allows you to focus your time on dreaming up in writing your next great application. This means less time worrying about infrastructure, managing and maintaining deployment environments, worrying about security or building and sharing your containers, and it allows you to focus on building your next idea for scale. In the next module, you'll learn about the architecture of Kubernetes.

Welcome to the Kubernetes Architecture module of our course. It helps to know the parts of Kubernetes and to understand the philosophy it implements. How does Kubernetes expect you to tell it what to do? What choices do you have for describing your workloads? That's the theme of this module. In this module, you'll learn how to, understand how the Kubernetes architecture is laid out, deploy a Kubernetes cluster using Google Kubernetes Engine, deploy pods to a GKE cluster, and view and managed several very useful kinds of Kubernetes objects.

In this lesson, we'll lay out the fundamental components of the Kubernetes operating philosophy. To understand how Kubernetes works, there are two related concepts you need to understand. The first is the Kubernetes object model. 
Each thing Kubernetes manages is represented by an object. You can view and change these objects, attributes, and state.
The second is the principle of declarative management, Kubernetes expects you to tell it what you want, the state of the objects under each management to be. It will work to bring that state into being and keep it there. 
Formally, a Kubernetes object is defined as a persistent entity that represents the state of something running in a cluster, it's desired state and its current state. Various kinds of objects represent containerized applications, the resources that are available to them, and the policies that affect their behavior. Kubernetes objects have two important elements. 
You give Kubernetes an objects spec for each object you wanted to create. With this spec, you define the desired state of the object by providing the characteristics that you want. 
The object's status is simply the current state of the object provided by the Kubernetes control plane. By the way, we use this term, Kubernetes control plane, to refer to the various system processes that collaborate to make a Kubernetes cluster work. You'll learn about these processes later in this module. 
Each object is of a certain type or kind, as Kubernetes calls them. Pods are the basic building block of the standard Kubernetes model, and they're the smallest deployable Kubernetes object. You may be surprised to hear me say that. Maybe you were expecting me to say that the smallest Kubernetes object is the container. Not so, every running container and Kubernetes system is in a pod. A pod embodies the environment where the containers live. 
That environment can accommodate one or more containers. If there is more than one container in a pod, they are tightly coupled and share resources including networking and storage. Kubernetes assigns each pod a unique IP address. Every container within a pod shares the network namespace, including IP address and network ports. Containers within the same pod can communicate through localhost. 
The famous 127.0.0.1 IP address that you pranked your friends with back in the 1990s. A pod can also specify a set of Storage volumes to be shared among its containers. By the way, later in this specialization, you'll learn how pods can share storage with one another, not just within a single pod. 
Let's consider a simple example where you want three instances of the NginX Web server, each in its own container, running all the time. 
How is this achieved in Kubernetes? Remember that Kubernetes embodies the principle of declarative management. You declare some objects to represent those NginX containers. What kind of object? Perhaps pods. Now it is Kubernetes job to launch those pods and keep them in existence. But be careful, pods are not self healing. If we want to keep all our NginX Web servers not just in existence, but also working together as a team, we might want to ask for them using a more sophisticated object. I'll tell you how later in this module. 
Let's suppose that we have given Kubernetes a desired state that consists of three NginX pods always kept running. We did this by telling Kubernetes to create and maintain one or more objects that represent them. Now, Kubernetes compares the desired state to the current state. Let's imagine that our declaration of three NginX containers is completely new. The current state does not match the desired state. Kubernetes, specifically it's control plane, will remedy the situation because the number of desired pods running we declared as three and zero while presently running, three will be launched. The Kubernetes control plane will continuously monitor the state of the cluster, endlessly comparing reality to what has been declared and remedying the state as needed.

In the previous lesson, I mentioned the Kubernetes control plane, which is the fleet of cooperating processes that make a Kubernetes cluster work. Even though you'll only work directly with a few of these components, it helps to know about them and the role each plays. I'll build up a Kubernetes cluster part by part, explaining each piece as I go. 
After I'm done, I'll show you how a Kubernetes cluster running in GKE is a lot less work to manage than one you've provisioned yourself. Okay, here we go. 
First and foremost, your cluster needs computers. Nowadays, the computers that compose your clusters are usually virtual machines. They always are in GKE, but they could be physical computers too. One computer is called the master and the others are called simply, nodes. 
The job of the nodes is to run pods. The job of the master is to coordinate the entire cluster. We will meet its control plane components first. Several critical Kubernetes components run on the master. The single component that you interact with directly is the kube-APIserver. 
This component's job is to accept commands that view or change the state of the cluster, including launching pods. In this specialization, you will use the kubectl command frequently. This command's job is to connect to kube-APIserver and communicate with it using the Kubernetes API. Kube-API server also authenticates incoming requests, determines whether they are authorized, invalid, and manages admission control. 
But it's not just kubectl that talks with kube-APIserver. In fact, any query or change to the cluster state must be addressed to the kube-APIserver. Etcd is the cluster's database. Its job is to reliably store the state of the cluster. This includes all the cluster configuration data and more dynamic information such as what nodes are part of the cluster, what pods should be running, and where they should be running. 
You never interact directly with etcd. Instead, kube-APIserver interacts with the database on behalf of the rest of the system. Kube-scheduler is responsible for scheduling pods onto the nodes. To do that, it evaluates the requirements of each individual pod and selects which node is most suitable. But it doesn't do the work of actually launching pods onto nodes. Instead, whenever it discovers a pod object that doesn't yet have an assignment to a node, it chooses a node and simply write the name of that node into the pod object. 
Another component of the system is responsible for then launching the pods, and you will see it very soon. But how does kube-scheduler decide where to run a pod? It knows the state of all the nodes, and it will also obey constraints that you define on where a pod may run, based on hardware, software, and policy. For example, you might specify that a certain pod is only allowed to run on nodes with a certain amount of memory. You can also define affinity specifications, which cause groups of pods to prefer running on the same node. 
Or anti-affinity specifications which ensure that pods do not run on the same node. You'll learn more about some of these tools in later modules. Kube-controller manager has a broader job. It continuously monitors the state of a cluster through kube-APIserver. Whenever the current state of the cluster doesn't match the desired state, kube-controller manager will attempt to make changes, to achieve the desired state. It's called the controller manager because many Kubernetes objects are maintained by loops of code called controllers. 
These loops of code handle the process of remediation. Controllers will be very useful to you. To be specific, you all use certain kinds of Kubernetes controllers to manage workloads. For example, remember our problem of keeping three engine x pods always running. 
We can gather them together into a controller object called a deployment, that not only keeps them running, but also lets us scale them and bring them together underneath our front end. We'll meet deployments later in this module. Other kinds of controllers have system-level responsibilities. For example, node controller's job is to monitor and respond when a node is offline. Kube-cloud-manager manages controllers that interact with underlying cloud providers. For example, if you manually launched a Kubernetes cluster on Google Compute Engine, kube-cloud-manager would be responsible for bringing in GCP features like load balancers and storage volumes when you needed them. 
Each node runs a small family of control-plane components too. For example, each node runs a kubelet. You can think of kubelet as Kubernetes agent on each node. When the kube-APIserver wants to start a pod on a node, it connects to that node's kubelet. Kubelet uses the container runtime to start the pod and monitor its lifecycle, including readiness and liveness probes, and reports back to kube-APIserver. 
Do you remember our use of the term container runtime in the previous module? This is the software that knows how to launch a container from a container image. The world of Kubernetes offers several choices of container runtimes, but the Linux distribution, that GKE uses for its nodes, launches containers using container D. The runtime component of docker. Kube proxies job is to maintain network connectivity among the pods in a cluster. In open-source Kubernetes, it does so using the firewalling capabilities of IP tables, which are built into the Linux kernel. Later in this specialization, we will learn how GKE handles pod networking.

Next, we'll introduce concepts specific to Google Kubernetes Engine. That diagram of the Kubernetes is control-plane had a lot of components, didn't it? 
Setting up a Kubernetes cluster by hand is tons of work. 
Fortunately, there's an open-source command called cubeadm that can automate much of the initial setup of a cluster. But if a node fails or needs maintenance, human administrator has to respond manually. I suspect you can see why a lot of people like the idea of a managed service for Kubernetes. 
You may be wondering how that picture we just saw differs for GKE. Well, here it is. From the user's perspective, it's a lot simpler. GKE manages all the control-plane components for us. 
It's still exposes an IP address to which we send all of our Kubernetes API requests. But GKE takes responsibility for provisioning and managing all the master infrastructure behind it. It also abstracts away having a separate master. The responsibilities of the master are absorbed by GCP, and you are not separately billed for your Master. 
Now let's talk about nodes. In any Kubernetes environment, nodes are created externally by cluster administrators, not by Kubernetes itself. GKE automates this process for you. 
It launches Compute Engine virtual machine instances and registers them as nodes. You can manage node settings directly from the GCP console. 
You pay per hour of life of your nodes, not counting the master. Because nodes run on Compute Engine, you choose your node machine type when you create your cluster. 
By default, the node machine type is N1 standard one, which provides one virtual CPU and 3.75 gigabytes of memory. Google Cloud offers a wide variety of Compute Engine options. At the time this video was made generally available maximum was 96 virtual CPU cores, that's a moderately big Virtual Machine. 
You can customize your nodes, number of cores, and their memory capacity. You can select a CPU Platform. You can choose a baseline minimum CPU platform for the nodes or node pool. This allows you to improve node performance. GKE will never use a platform that is older than the CPU platform you specify. 
If it picks a newer platform, the cost will be the same as the specified platform. You can also select multiple node machine types by creating multiple node pools. 
A node pool is a subset of nodes within a cluster that share a configuration, such as their amount of memory or their CPU generation. Node pools also provide an easy way to ensure that workloads run on the right hardware within your cluster. You just label them with a desired node pool. 
By the way, node pool are GKE feature rather than a Kubernetes feature. You can build an analogist mechanism within open-source Kubernetes, but you would have to maintain it yourself. You can enable automatic node upgrades, automatic node repairs, and cluster auto-scaling at this node pool level. 
Here's a word of caution. Some of each node CPU and memory are needed to run the GKE and Kubernetes components that let it work as part of your cluster. For example, if you allocate nodes with 15 gigabytes of memory, not quite all of that 15 gigabytes will be available for use by pods. This module has a documentation link that explains how much CPU and memory are reserved. By default, a cluster launches in a single GCP Compute Zone with three identical nodes, all in one node pool. The number of nodes can be changed during or after the creation of the cluster. 
Adding more nodes and deploying multiple replicas of an application will improve an applications availability, but only up to a point. What happens if the entire Compute Zone goes down? You can address this concern by using a GKE regional cluster. Regional clusters have a single API endpoint for the cluster. However, it's masters and nodes are spread out across multiple Compute Engine zones within a region. 
Regional clusters ensure that the availability of the application is maintained across multiple zones in a single region. In addition, the availability of the master is also maintained so that both the application and management functionality can withstand the loss of one or more, but not all zones. By default are regional cluster is spread across three zones, each containing one master and three nodes. 
These numbers can be increased or decreased. For example, if you have five nodes in zone one, you will have exactly the same number of nodes in each of the other zones for a total of 15 nodes. Once you build a zonal cluster, you can't convert it into a regional cluster or vice versa. Regional and zonal GKE clusters can also be setup as a private cluster. The entire cluster that is the master and it's nodes are hidden from the public Internet. Cluster masters can be accessed by Google Cloud products such as Stack driver through an internal IP address. 
They can also be accessed by authorized networks through an external IP address. Authorize networks are basically IP address ranges that are trusted to access the master.
In addition, nodes can have limited outbound access through private Google access, which allows them to communicate with other GCP services. For example, nodes can pull Container images from Google Container Registry without needing external IP addresses. The topic of private clusters is discussed in more detail in another module in this specialization.

Now we'll discuss Kubernetes Object Management.
All Kubernetes objects are identified by a unique name and a unique identifier. Let's return once again to our example in which we want three nginx web servers running all the time. 
Well, the simplest way would be to declare three pod objects and specify their state. For each, a pod must be created and an nginx container image must be used. Let's see how we declare this. 
You define the objects you want Kubernetes to create and maintain with manifest files. These are ordinary text files. You may write them in YAML or JSON format. YAML is more human-readable and less tedious to edit and we will use it throughout this specialization. This YAML file defines a desired state for a pod, its name, and a specific container image for it to run. Your manifest files have certain required fields. API version describes which Kubernetes API version is used to create the object. The Kubernetes protocol is versioned so as to help maintain backwards compatibility. Kind defines the object you want, in this case a pod. Metadata helps identify the object using name, unique ID, and an optional namespace. 
You can define several related objects in the same YAML file and it is a best practice to do so. One file is often easier to manage than several. Another even more important tip. You should save your YAML files in version controlled repositories. 
This practice makes it easier to track and manage changes and to back out those changes when necessary. It's also a big help when you need to recreate or restore a cluster. 
Many GCP customers use Cloud Source Repositories for this purpose because that service lets them control their permissions of those files in the same way as their other GCP resources. When you create a Kubernetes object, you name it with a string. Names must be unique. Only one object of a particular kind can have a particular name at the same time in a Kubernetes namespace. However, if an object is deleted, it's name can be reused. Alphanumeric characters, hyphens and periods are allowed in the names with a maximum character length of 253. 
Each object generated throughout the life of a cluster has a unique ID generated by Kubernetes. This means that no two objects will have the same UID throughout the life of a cluster. Labels are key value pairs with which you tag your objects during or after their creation. Labels help you identify and organize objects and subsets of objects. For example, you could create a label called app and give as its value, the application of which this object is a part. In this simple example, a deployment object is labeled with three different key values. 
It's application, it's environment, and which stack it forms a part of. Various contexts offer ways to select Kubernetes resources by their labels. In this specialization, you will spend plenty of time with the kubectl command. 
Here's an example of using it to show all the pods that contain a label called app with a value of nginx. Label selectors are very expressive. You can ask for all the resources that have a certain value for a label, all those that don't have a certain value, or even all those that have a value in a set you supply. 
One way to bring three nginx web servers into being, would be to declare three pod objects, each with its own section of YAML. Kubernetes default scheduling algorithm prefers to spread the workload evenly across the nodes available to it. We'd get a situation like this one. Looks good, doesn't it? Maybe not. Suppose I want 200 more nginx instances. Managing 200 more sections of YAML sounds very inconvenient. 
Here's another problem. Pods don't heal or repair themselves and they're not meant to run forever. They are designed to be ephemeral and disposable. For these reasons, there are better ways to manage what you run in Kubernetes than specifying individual pods. You need a set up like this to maintain an application's high availability along with horizontal scaling. How do you tell Kubernetes to maintain the desired state of three nginx containers? We can instead declare a controller object whose job is to manage the state of the pods. 
Some examples of these objects, Deployments, StatefulSets, DaemonSets, and Jobs. We'll meet all of these in our specialization. Deployments are a great choice for long lives software components, like web servers, especially when we want to manage them as a group. 
In our example, when kube-scheduler, schedules pods for a deployment, it notifies the kube-IP server. These changes are constantly monitored by controllers, especially by the deployment controller. The deployment controller will monitor and maintain three nginx Pods. If one of those pods fails, the deployment controller will recognize the difference between the current state and the desired state, and we'll try to fix it by launching a new pod. 
Instead of using multiple YAML manifests or files for each pod, you used a single deployment YAML to launch three replicas of the same container. A deployment ensures that a defined set of pods is running at any given time. Within its objects spec, you specify how many replica pods you want, how pods should run, which containers should run within these pods, and which volumes should be mounted. Based on these templates, controllers maintain the pods desired state within a cluster. 
Deployments can also do a lot more than this, which you will see later in the course. It's very probable that you'll be using a single cluster for multiple projects. 
At the same time, it's essential to maintain resource quotas based on projects or teams. By the way, when I say projects here, I mean projects in the informal sense of the word. Things you and your colleagues are working on. Each Kubernetes clusters associated with one GCP project in the formal sense of the word Project. That's how I'm policies apply to it and how you're built for it. How do you keep everybody's work on your cluster, tidy and organized? Kubernetes allows you to abstract a single physical cluster into multiple clusters known as namespaces. 
Namespaces provides scope for naming resources such as pods, deployments, and controllers. As you can see in this example, there are three namespaces in this cluster; test, stage, and prod. Remember that you cannot have duplicate object names in the same namespace. You can create three pods with the same name in genetics in this case, but only if they don't share the same namespace. If you attempt to create another pod with the same name, nginx Pod in namespace test, you won't be allowed. Object names need only be unique within a namespace, not across all namespaces. 
Namespaces also led to implement resource quotas across the cluster. These quotas defined limits for resource consumption within a namespace. They're not the same as your GCP quotas, which we discussed in an earlier module. These quotas apply specifically to the Kubernetes cluster they are defined on. You're not required to use namespaces for your day to day management. You can also use labels. Still namespaces are a valuable tool. 
Suppose you want to spin up a copy of a deployment as a quick test. Doing so in a new namespace makes it easy and free of name collisions. There are three initial namespaces in a cluster. The first is a default namespace, for objects with no other namespace defined. Your Workload resources will use this namespace by default. Then there is the kube-system namespace, for objects created by the Kubernetes system itself. 
We'll see more of the object kinds in this diagram elsewhere in this specialization. When you use the kubeCTL command, by default, items in the kube-system namespace are excluded, but you can choose to view its contents explicitly. The third namespaces that kube-public namespace, for objects that are publicly readable to all users. Kube-public is a tool for disseminating information to everything running in a cluster. 
You're not required to use it, but it can come in handy, especially when everything running in a cluster is related to the same goal and needs information in common. You can apply a resource to a namespace when creating it using a command line namespace flag. Or you can specify a namespace in the YAML file for the resource. Whenever possible, apply namespaces at the command line level. This practice makes your YAML files more flexible. For example, someday, you might want to create two completely independent instances of one of your deployments, each in its own namespace. This is difficult if you've chosen to embed namespace names in your YAML files.

Remember that Pods are created and destroyed dynamically. Although Pods can communicate using their assigned Pod IP addresses, these IP addresses are ephemeral. 
They're not guaranteed to remain constant when Pods are restarted or when scaling changes which nodes are used to run Pods. Imagine you have two sets of Pods, frontend Pods, and backend Pods. How will the frontend Pods discover and keep track of dynamically scaling backend Pods? 
This is where the concept of Kubernetes Services comes in. A Kubernetes service is a static IP address that represents a service or a function in your infrastructure. It's a network abstraction for a set of Pods that deliver that service, and it hides the ephemeral nature of the IP addresses of the individual Pods. In the example, a set of backend Pods are exposed to the frontend Pod using a Kubernetes service. 
Basically, the service defines a set of Pods and assigns a policy by which you can access those Pods. The Pods are selected using a label selector. By the way, you can also get a service quickly by asking Kubernetes to expose a deployment. When you do that Kubernetes handles selecting the right Pods for you. Whenever a service is created, Kubernetes automatically creates endpoints for the selected Pods, by creating endpoint resources.
By default, the master assigns a virtual IP address, also known as a cluster IP, to the service from internal IP tables. With GKE, this is assigned from the clusters VPC network. You will learn more about services in a later module in this specialization. GKE offers other ways your service can be exposed, not just through cluster IPs. Overall, a service provides durable endpoints for Pods. 
These endpoints can be accessed by exposing the service internally within a cluster, or externally to the outside world. The option to expose a service internally or externally depends on the service type itself. The frontend Pod can reliably access the backend Pods internally within the cluster using a service. A Container application can easily write data to the read-write layer inside the container, but it's ephemeral. 
When the container terminates, whatever was written will be lost. What if you want to store data permanently? Or what if you need storage to be shared between tightly coupled containers within a Pod? That's why a Kubernetes volume is used for more persistent storage. Kubernetes' volume is another abstraction. A volume is simply a directory that is accessible to all the containers in a pod. The requirements for a volume are defined through the pods' specification. This declares how the directory is created, what storage medium should be used, and its initial contents. You don't want failure of containers or restarts of containers to affect the data within these volumes, and you want your volume to be shared among multiple containers within a Pod. Docker containers have their own file system. 
Therefore, in order to access these volumes, they must be mounted specifically on each container within a Pod. However, Pods themselves are also ephemeral. A failing node or deleted Pod could lead to its volume being deleted too. To avoid this, you can configure volumes using network-based storage from outside of your Pods to provide durable storage that is not lost when a Pod or node fails. You'll learn about persistent volumes later in this specialization.

Services provide load-balanced access to specified Pods. There are three primary types of Services:

ClusterIP: Exposes the service on an IP address that is only accessible from within this cluster. This is the default type.
NodePort: Exposes the service on the IP address of each node in the cluster, at a specific port number.
LoadBalancer: Exposes the service externally, using a load balancing service provided by a cloud provider. 
In Google Kubernetes Engine, LoadBalancers give you access to a regional Network Load Balancing configuration by default. To get access to a global HTTP(S) Load Balancing configuration, you can use an Ingress object.

You will learn more about Services and Ingress objects in a later module in this specialization.

This reading explains the relationship among several Kubernetes controller objects:

ReplicaSets
Deployments
Replication Controllers
StatefulSets
DaemonSets
Jobs

A ReplicaSet controller ensures that a population of Pods, all identical to one another, are running at the same time. Deployments let you do declarative updates to ReplicaSets and Pods. In fact, Deployments manage their own ReplicaSets to achieve the declarative goals you prescribe, so you will most commonly work with Deployment objects.

Deployments let you create, update, roll back, and scale Pods, using ReplicaSets as needed to do so. For example, when you perform a rolling upgrade of a Deployment, the Deployment object creates a second ReplicaSet, and then increases the number of Pods in the new ReplicaSet as it decreases the number of Pods in its original ReplicaSet.

Replication Controllers perform a similar role to the combination of ReplicaSets and Deployments, but their use is no longer recommended. Because Deployments provide a helpful "front end" to ReplicaSets, this training course chiefly focuses on Deployments.

If you need to deploy applications that maintain local state, StatefulSet is a better option. A StatefulSet is similar to a Deployment in that the Pods use the same container spec. The Pods created through Deployment are not given persistent identities, however; by contrast, Pods created using StatefulSet have unique persistent identities with stable network identity and persistent disk storage. 

If you need to run certain Pods on all the nodes within the cluster or on a selection of nodes, use DaemonSet. DaemonSet ensures that a specific Pod is always running on all or some subset of the nodes. If new nodes are added, DaemonSet will automatically set up Pods in those nodes with the required specification. The word "daemon" is a computer science term meaning a non-interactive process that provides useful services to other processes. A Kubernetes cluster might use a DaemonSet to ensure that a logging agent like fluentd is running on all nodes in the cluster.

The Job controller creates one or more Pods required to run a task. When the task is completed, Job will then terminate all those Pods. A related controller is CronJob, which runs Pods on a time-based schedule.

Later modules in this specialization will cover these controllers in more depth.

In this lab, you'll build and use GKE clusters and deploy a sample pod. The tasks that you'll learn to perform include using the GCP Console to build and manipulate GKE clusters, deploy a pod, and examine the cluster and pods. Now, go ahead and try the quick lab. You have multiple attempts to complete it, so don't worry if you need more time or want to practice more. Afterward, come back here and we'll walk through the lab solution and key learning points together.

Let's start by confirming that the Kubernetes engine API is enabled.
In the APIs and services area of the GCP Console, we click, enable APIs and services, and search for Kubernetes engine.
It's enabled, if it had not been enabled there would have been a button to enable.
Now, let's make a Kubernetes engine cluster.
Let's take all the defaults. We're going to build a zonal cluster named standard cluster one, in the US-Central-1-a zone. We'll have one node pool with three nodes. We click, create.
It takes a few minutes for Kubernetes engine to build the cluster.
Because we chose a zonal cluster composed of N1 standard one nodes, the total cluster size is three virtual CPUs and just over 11 gigabytes of memory. Let's examine the details by clicking on the cluster name. We'll learn about cluster storage later in this specialization. Here are the nodes, each has a computer engine virtual machine instance.
Let's click edit and see what we can change on an already created cluster.
Many attributes of the cluster can be changed or expanded. Let's change the size of the default node pool to four nodes.
As with the cluster creation, expansion takes a few minutes.
Now, the cluster has additional capacity, later in this specialization, you will learn how to auto-scale the cluster capacity. Now, let's use GCP console to deploy a workload to the cluster. In this example, we'll use the nginx web server.
We'll accept the defaults for its deployment,
and we'll launch the deployment.
The deployment is done. Later in this specialization, we will learn how to expose it as a service, so the traffic can reach our ngnix web servers. Let's examine the deployment by clicking on its name. Deployment is new, so the graphs are not yet populated.
We will learn about labels, annotations, update strategies, pod specifications and the horizontal pod auto scaler, later in this specialization.
Also see the deployments revision history, events in its life and the YAML that defines the deployment.
Let's return to the overview tab, and scroll down to the managed pods section. Here's information about one of the pods that makes up the deployment.
YAML specifications are a very powerful way to define Kubernetes objects. You'll learn more in this specialization.

Hi, I'm Eoin. So you've heard a bit about Kubernetes and it sounds pretty great. But what if you've got existing applications, that are not in containers, or perhaps not even in the cloud? Well, don't worry, because Google Cloud is the solution for this. Migrate for Anthos, is our tool for getting workloads into containerized deployments on Google Cloud. Let's look at what Migrate for Anthos does. Migrate for Anthos, moves your existing applications into a Kubernetes environment. And the best thing about this, is the process is automated. Your workloads can be on premises, or in other cloud providers. I've already mentioned that Migrate for Anthos are automated, but it's also very fast. Most migrations are completed in less than 10 minutes. And, you have the choice of migrating your applications data in one move, or stream it to the cloud until the application is live. That sounds pretty amazing. Let's have a quick overview of what happens in a migration.

First, let's inspect the architecture required for a Migration. The first step is to allow Migrate for Compute Engine to create a pipeline for Streaming or Migration the data from on-premises or another cloud provider into Google Cloud. Migrate for Compute Engine is a tool that allows you to bring your existing applications into VMs on Google Cloud. Migrate from [inaudible] is then installed on a GKE processing cluster and is composed of many Kubernetes resources. Migrate from [inaudible] is used to generate Deployment Artifacts. With these artifacts, like your Kubernetes configurations in the Docker File are used to create the VM Wrapping Container. This Container goes into Cloud Storage. The Container Images themselves are stored in the Container Registry. After the deployment assets are created, they can be used to deploy your application into a target cluster. You simply apply the generator configuration and it creates all the necessary Kubernetes elements on the target cluster.

Now that you've seen the architecture required from migration, let's look at what happens when you migration application using Migrate For Anthos. First you need to create the processing cluster. After that you install the Migrate For Anthos components onto that cluster. Next you need to add a migration source. You can migrate from VMware, AWS, Azure or Google cloud. You will need to create a migration object with the details of the migration that you're performing. This will generate a plan template for you in a YAML file. You may need to alter this configuration file to create the level of customization that you desire. When the plan is ready, you will need to generate the artifacts for the migration. This means generation that container images of your applications on the YAML files required for the deployment. After your migration artifacts have been generated, they need to be tested. Both the container images and the deployments will be tested at this stage. Finally, if the tests are successful, you can use the generative artifacts to deploy your application in to your production clusters.

OK, we've looked at the infrastructure required for using my great friend us on the basic path that the migration journey would take. Now let's look at a worked example of installing the necessary tools to perform our migration. Let's go to an example and see what happens at each stage. The first thing you need to do is set up the processing cluster before you run the command on screen. You need to make sure that you are a GKE admin to set up the cluster. You also must have firewall rules in place to allow communications between migrate for Anthos and migrate for compute engine. After all that's done, you can create the processing cluster. The example on screen enables a VPC native cluster. When the processing cluster is up and running, you need to install migrate for Anthos using the make CDL Command. This command installs all of the required Kubernetes resources onto the processing cluster for the migration. The mix ETL source create command specifies the location of the application to migrate. The example on screen is from migrating from Google compute engine. If you're migrating from a VM Ware back end or another cloud provider, you need to install some additional packages. Now that the infrastructure elements are set up, the next step is to create a migration plan. The mix ETL migration create command will create the migration plan. This command will define the migration resources to be created on the cluster. You identify the source VM on what data to exclude from the migration. You can also specify what migration intends you want. You can specify the following intents, image, image and data, data or PV based container. The output of this command is a Yamal file that can be further customized. After creating a migration plan, you will need to generate the artifacts for the migration. The mix ETL migration generates artifacts command on screen will start this process. This process will first copy files and directories representing the VM to a container image registry as images. Migrate for Anthos creates 2 images, a runnable image for deployment into another cluster and a non runnable image layer that can be used to update the container image in the future. Next migrate for Anthos will generate configuration Yamal files. That you can use to deploy the VM to another GK Eve cluster. These are copied into a cloud storage book, as an intermediate location. You run the mix ETL migration, get artifacts command to download the Yamal configuration file generated from the last step. The Yammer configuration defines the resources to deploy, such as are you creating a deployment or a stateful set. Is the deployment headless service? Are you using persistent volumes or persistent volume claims? You can edit the Mo file to customize your deployment. Examples of customizations include enabling load balancing, allowing Ingress, or defining this size. Finally, you run the kubectl apply command to deploy to define specification.

This concludes the Kubernetes architecture module. In this module, you learned about the Kubernetes operating philosophy. Every item under Kubernetes control is represented by an object. And Kubernetes tries to keep the state of its cluster matching the state you have declared that you want.
You learned about the control playing components that make up Kubernetes. You learned about Cube API server, which is the point of control for your Kubernetes cluster. And you learned about Cubelet, which is your cluster's agent on each node.
And you also learned that a GKE managed cluster implements the master for you, behind the scenes, and that you are not charged separately for it.
To tell Kubernetes what you want the state of your cluster to be, create manifest files. Typically, you'll build those files in YAML format. These files name and describe the objects you want Kubernetes to keep alive and healthy. They also documented the desired state of your cluster. So you should keep these files in a source control system.